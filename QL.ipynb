{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from system import (\n",
    "    states,\n",
    "    actions, \n",
    "    wind_col,\n",
    "    num_states,\n",
    "    num_actions,\n",
    "    init_mdp,\n",
    "    init_reward,\n",
    "    to_idx,\n",
    "    to_state,\n",
    "    get_valid_actions,\n",
    "    gen_random_sa,\n",
    "    rows,\n",
    "    cols\n",
    ")\n",
    "from model import Model\n",
    "from policy import Policy, PolicyInit\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = init_mdp(num_states, num_actions, wind_col)\n",
    "reward = init_reward(num_states, t_reward=100)\n",
    "model = Model(mdp, reward)\n",
    "\n",
    "policy = Policy(num_states, num_actions, PolicyInit.RANDOM)\n",
    "\n",
    "agent = Agent(model, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ch_egreedy(epsilon, Q, s):\n",
    "    p = np.zeros(num_actions)\n",
    "    valid_actions_idx = get_valid_actions(to_state(s), idx=True)\n",
    "    va = len(valid_actions_idx)\n",
    "    # print(va)\n",
    "    p[valid_actions_idx] = epsilon / va\n",
    "    validqsa = Q[s][valid_actions_idx]\n",
    "    p[valid_actions_idx[np.argmax(validqsa)]] += 1 - epsilon\n",
    "    choice = np.random.choice(np.arange(num_actions), p=p)\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1]\n",
      "-2.5043749363922023\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m     nmax\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(validqsa)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(nmax)\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mQ_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m Q_val[state, action] \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m (r \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m nmax \u001b[38;5;241m-\u001b[39m Q_val[state, action])\n\u001b[0;32m     26\u001b[0m     state \u001b[38;5;241m=\u001b[39m to_idx(nstate)\n\u001b[0;32m     27\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "Q_val = np.random.uniform(-10, -1, (num_states, num_actions))\n",
    "for a in range(num_actions):\n",
    "    Q_val[37][a] = 0\n",
    "# Q_val = np.zeros((num_states, num_actions))\n",
    "returns = [[[] for a in range(num_actions)] for s in range(num_states)]\n",
    "valf_list = [(np.max(Q_val, axis=1))]\n",
    "\n",
    "i = 0\n",
    "T = 1000\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "while i < 1000:\n",
    "    start_state = 30\n",
    "    state = start_state\n",
    "    while state != 37:\n",
    "        action = ch_egreedy(0.1, Q_val, state)\n",
    "        # print(action)\n",
    "        nstate = model.gen_next(to_state(state), action)\n",
    "        r = model.get_reward(to_state(state), to_state(nstate))\n",
    "        print(r)\n",
    "        valid_actions_idx = get_valid_actions(nstate, idx=True)\n",
    "        validqsa = Q_val[to_idx(nstate)][valid_actions_idx]\n",
    "        nmax= np.max(validqsa)\n",
    "        print(nmax)\n",
    "        Q_val[state, action] = Q_val[state, action] + alpha * (r + gamma * nmax - Q_val[state, action])\n",
    "        state = to_idx(nstate)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valf = valf_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "valf_toplot = valf.reshape(rows, cols)\n",
    "plt.gca().invert_yaxis()\n",
    "heatmap = plt.imshow(valf_toplot)\n",
    "plt.colorbar(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent\n",
    "score = 0\n",
    "steps = 0\n",
    "s = np.array([3,0])\n",
    "\n",
    "path = [s]\n",
    "\n",
    "while(steps < 1000):\n",
    "    a = policy.get_action(s)\n",
    "\n",
    "    s_ = model.gen_next(s, a)\n",
    "    path.append(s_)\n",
    "\n",
    "    r = model.get_reward(s, s_)\n",
    "    score += r\n",
    "    print(f\"State: {s}, Action: {actions[a]}, Next State: {s_}, Reward: {r}\")\n",
    "\n",
    "    if(r!= -1):\n",
    "        print(f\"Game Over - Score: {score}\")\n",
    "        break\n",
    "    \n",
    "    s = s_\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_plt = np.array(path) + 0.5\n",
    "y, x = path_plt.T\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.grid()\n",
    "\n",
    "ax.set_xlim(0, cols)\n",
    "ax.set_xticks(np.arange(0, cols, 1), minor=False)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xticks(np.arange(0.5, cols, 1), minor=True)\n",
    "ax.set_xticklabels([str(x) for x in wind_col], minor=True)\n",
    "\n",
    "ax.set_ylim(rows, 0)\n",
    "ax.set_yticks(np.arange(0, rows, 1), minor=False)\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "ax.plot(x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
